{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipes for data import and cleanup\n",
    "\n",
    "In my experience, around 75% of the time you spend working with data will be fighting to import it and clean it up. For the most part this is just general-purpose programming, but there are a few library routines that will save you from reinventing the wheel.\n",
    "\n",
    "* [Preamble](#preamble)\n",
    "* [Reading from a csv file](#readfile)\n",
    "* [Reading from a string](#readstr)\n",
    "* [Reading from a url or string](#readurl)\n",
    "* [Parsing a log file with regular expressions](#regexp)\n",
    "* [Reading JSON from a web service](#json)\n",
    "* [Scraping a website with xpath](#xpath)\n",
    "* [Querying an SQL database](#sql)\n",
    "\n",
    "Treat this section as a collection of recipes and pointers to useful library routines. If you find yourself needing them, you should read the recipe, try it out, then look online for more information about the library functions it suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a csv file <span id=\"readfile\"></span>\n",
    "\n",
    "When our data is a very simple comma-separated value (CSV) file then it's very easy to import with `pandas.read_csv`. We can specify either a filename or a url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our file is nearly a CSV but has some quirks such as comments or a missing header row, there are plenty of options in [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) or [`pandas.read_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html). For extreme quirks we may need to use the raw Python [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from a string<span id=\"readstr\"></span>\n",
    "\n",
    "If we have a string that we want to treat as a file, we can use `io.StringIO`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "data = '''\n",
    "x,y\n",
    "10,3\n",
    "2,5\n",
    "'''\n",
    "\n",
    "df = pandas.read_csv(io.StringIO(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from an http request<span id=\"readurl\"></span>\n",
    "\n",
    "If we want to read from a url but we want more control over the http request, for example sending a POST request or modifying the request header or reading the response header, we can use the `requests` library to fetch the data as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "my_url = \"https://www.cl.cam.ac.uk/teaching/2122/DataSci/data/iris.csv\"\n",
    "data = requests.get(my_url).content.decode('utf8')\n",
    "df = pandas.read_csv(io.StringIO(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing log files with regular expressions<span id=\"regexp\"></span>\n",
    "A typical line from a web server log might look like this\n",
    "```\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" 200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "```\n",
    "where (according to the [Apache web server documentation](https://httpd.apache.org/docs/2.4/logs.html#combined)) the pieces are\n",
    "\n",
    "* **`207.46.13.169`** \n",
    "The IP address that made the request\n",
    "* **`-`** \n",
    "The identity of the client; `-` means not available\n",
    "* **`-`**\n",
    "The userid of the logged-in user who made the request; `-` means not available\n",
    "* **`[27/Aug/2017:06:52:11 +0000]`**\n",
    "The time the request was received\n",
    "* **`\"GET /marcus/essay/st&h2.html HTTP/1.1\"`**\n",
    "The type of request, what was requested, and the connection type\n",
    "* **`200`**\n",
    "The [http status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) (200 means OK)\n",
    "* **`3881`**\n",
    "The size of the object returned to the client, in bytes\n",
    "* **`-`**\n",
    "The referrer URL; `-` means not available\n",
    "* **`\"Mozilla/5.0 (...)\"`**\n",
    "The browser type. The substring `bingbot` here tells us that the request comes from Microsoft Bing's web crawler.\n",
    "\n",
    "To extract these pieces from a single line of the log file, the best tool is [regular expressions](https://docs.python.org/3.4/library/re.html), a mini-language for string matching that is common across many programming languages. The syntax is terse and takes a lot of practice. I like to start with a small string pattern and incrementally build it up, testing as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 16), match='\\n207.46.13.169 -'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re    # standard Python module for regular expressions\n",
    "\n",
    "s = \"\"\"\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" \n",
    "200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\n",
    "AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\n",
    "(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "\"\"\"\n",
    "\n",
    "# First attempt: match the first two items in the log line.\n",
    "# If my pattern is right, re.match returns an object.\n",
    "# If my pattern is wrong, re.match returns None.\n",
    "pattern_test = r'\\s*(\\S+)\\s*(\\S+)'\n",
    "re.match(pattern_test, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ip': '207.46.13.169',\n",
       " 'client': '-',\n",
       " 'user': '-',\n",
       " 't': '27/Aug/2017:06:52:11 +0000',\n",
       " 'req': 'GET /marcus/essay/st&h2.html HTTP/1.1',\n",
       " 'status': '200',\n",
       " 'size': '3881',\n",
       " 'ref': '-',\n",
       " 'ua': 'Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\\nAppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\\n(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the full pattern I built up to. Python lets us add verbose comments\n",
    "# to the pattern, which is handy for remembering what your code does when you look\n",
    "# at it the next morning.\n",
    "pattern = r'''(?x)  #   flag saying that this pattern has comments\n",
    "\\s*                 #   any whitespace at the start of the string\n",
    "(?P<ip>\\S+)         # one or more non-space characters: the IP address\n",
    "\\s+                 #   one or more spaces\n",
    "(?P<client>\\S+)     # the client identity\n",
    "\\s+\n",
    "(?P<user>\\S+)       # the userid\n",
    "\\s+\n",
    "\\[(?P<t>[^\\]]*)\\]   # [, then any number of not-] characters, then ]: the timestamp\n",
    "\\s+\n",
    "\"(?P<req>[^\"]*)\"    # \", then any number of not-\" characters, then \": the request string\n",
    "\\s+\n",
    "(?P<status>\\d+)     # one or more numerical digits: the http status code\n",
    "\\s+\n",
    "(?P<size>\\d+)       # one or more numerical digits: the size\n",
    "\\s+\n",
    "\"(?P<ref>[^\"]*)\"    # the referrer URL\n",
    "\\s+\n",
    "\"(?P<ua>[^\"]*)\"     # the user agent i.e. browser type\n",
    "'''\n",
    "m = re.match(pattern, s)\n",
    "m.groupdict()       # returns a dictionary of all the named sub-patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the fields from a full log file? The vanilla Python code is\n",
    "```\n",
    "with open(myfile) as f:\n",
    "    for line in f:\n",
    "        m = re.match(pattern, line)\n",
    "        # store the fields from m.groups() or m.groupdict() somewhere appropriate\n",
    "```\n",
    "Alternatively, `numpy` has a handy shortcut for reading in an entire file and splitting it via a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ip</th>\n",
       "      <th>client</th>\n",
       "      <th>user</th>\n",
       "      <th>t</th>\n",
       "      <th>req</th>\n",
       "      <th>status</th>\n",
       "      <th>size</th>\n",
       "      <th>ref</th>\n",
       "      <th>ua</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>82.132.187.229</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:10:43:19 +0000</td>\n",
       "      <td>GET /apple-touch-icon-120x120.png HTTP/1.1</td>\n",
       "      <td>404</td>\n",
       "      <td>517</td>\n",
       "      <td>-</td>\n",
       "      <td>MobileSafari/602.1 CFNetwork/811.5.4 Darwin/16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>207.46.13.169</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:08:09:13 +0000</td>\n",
       "      <td>GET /irene/cross/4-small.jpg HTTP/1.1</td>\n",
       "      <td>200</td>\n",
       "      <td>4784</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 (compatible; bingbot/2.0; +http://...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>46.176.68.80</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:10:59:29 +0000</td>\n",
       "      <td>HEAD http://54.246.111.98:80/mysql/mysqlmanage...</td>\n",
       "      <td>404</td>\n",
       "      <td>218</td>\n",
       "      <td>-</td>\n",
       "      <td>Mozilla/5.0 Jorgee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>1.129.97.177</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>27/Aug/2017:10:31:52 +0000</td>\n",
       "      <td>GET /apple-touch-icon-120x120-precomposed.png ...</td>\n",
       "      <td>404</td>\n",
       "      <td>525</td>\n",
       "      <td>-</td>\n",
       "      <td>MobileSafari/602.1 CFNetwork/811.5.4 Darwin/16...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ip client user                           t  \\\n",
       "821  82.132.187.229      -    -  27/Aug/2017:10:43:19 +0000   \n",
       "250   207.46.13.169      -    -  27/Aug/2017:08:09:13 +0000   \n",
       "879    46.176.68.80      -    -  27/Aug/2017:10:59:29 +0000   \n",
       "767    1.129.97.177      -    -  27/Aug/2017:10:31:52 +0000   \n",
       "\n",
       "                                                   req status  size ref  \\\n",
       "821         GET /apple-touch-icon-120x120.png HTTP/1.1    404   517   -   \n",
       "250              GET /irene/cross/4-small.jpg HTTP/1.1    200  4784   -   \n",
       "879  HEAD http://54.246.111.98:80/mysql/mysqlmanage...    404   218   -   \n",
       "767  GET /apple-touch-icon-120x120-precomposed.png ...    404   525   -   \n",
       "\n",
       "                                                    ua  \n",
       "821  MobileSafari/602.1 CFNetwork/811.5.4 Darwin/16...  \n",
       "250  Mozilla/5.0 (compatible; bingbot/2.0; +http://...  \n",
       "879                                 Mozilla/5.0 Jorgee  \n",
       "767  MobileSafari/602.1 CFNetwork/811.5.4 Darwin/16...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the file into an array, one row per line, one column per field in the pattern\n",
    "df = np.fromregex('data/webaccess_short.log', pattern, dtype=np.unicode_)\n",
    "\n",
    "# Make a dictionary out of the columns, according to the named fields in the pattern\n",
    "df = pandas.DataFrame({v: df[:,i-1] for v,i in re.compile(pattern).groupindex.items()})\n",
    "\n",
    "df.sample(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading json from a web data service<span id=\"json\"></span>\n",
    "More and more forward-thinking companies and government services make data available by simple web requests. Here is an example, stop-and-search data from [data.police.uk](https://data.police.uk/).\n",
    "\n",
    "The first step is to import the Python module for making web requests. When I'm developing data code I like to build it up in small steps, which means lots of repeated requests, so I also like to use another Python module which caches responses. This means I don't hammer the service unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, requests_cache\n",
    "requests_cache.install_cache('data/stopsearch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [API documentation](https://data.police.uk/docs/) tells us the URL for fetching a list of available data. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the request. This should print out <Response [200]>, meaning successfully retrieved\n",
    "AVAILABILITY_URL = 'https://data.police.uk/api/crimes-street-dates'\n",
    "stations_resp = requests.get(AVAILABILITY_URL)\n",
    "stations_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the body of the response. It's likely to be very long, so we'll only print out the first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"date\":\"2021-10\",\"stop-and-search\":[\"avon-and-somerset\",\"bedfordshire\",\"btp\",\"cambridgeshire\",\"cheshire\",\"city-of-london\",\"cleveland\",\"cumbria\",\"derbyshire\",\"devon-and-cornwall\",\"dorset\",\"durham\",\"dyfed-powys\",\"essex\",\"gloucestershire\",\"gwent\",\"hertfordshire\",\"humberside\",\"kent\",\"lancashire\",\"leic'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_resp.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like [JSON](https://en.wikipedia.org/wiki/JSON), \"JavaScript Object Notation\", a common format for web data services. \n",
    "\n",
    "It's easy to convert it into Python dictionaries and lists, with `requests.get(...).json()`. Now we can explore what it contains. (Alternatively, just read the web service documentation, if we trust it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'dict'>\n",
      "dict_keys(['date', 'stop-and-search'])\n",
      "{'date': '2021-10', 'stop-and-search': ['avon-and-somerset', 'bedfordshire', 'btp', 'cambridgeshire', 'cheshire', 'city-of-london', 'cleveland', 'cumbria', 'derbyshire', 'devon-and-cornwall', 'dorset', 'durham', 'dyfed-powys', 'essex', 'gloucestershire', 'gwent', 'hertfordshire', 'humberside', 'kent', 'lancashire', 'leicestershire', 'merseyside', 'metropolitan', 'norfolk', 'north-wales', 'north-yorkshire', 'northamptonshire', 'northumbria', 'nottinghamshire', 'south-wales', 'south-yorkshire', 'staffordshire', 'suffolk', 'surrey', 'sussex', 'thames-valley', 'warwickshire', 'west-mercia', 'west-yorkshire']}\n"
     ]
    }
   ],
   "source": [
    "x = requests.get(AVAILABILITY_URL).json()\n",
    "\n",
    "print(type(x))      # x = [...]\n",
    "print(type(x[0]))   # x = [{...}, ...]\n",
    "print(x[0].keys())  # x = [{date, stop-and-search}, ...]\n",
    "print(x[0])         # x = [{date, stop-and-search:[area,area,...]}, ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has a pretty flexible command for converting nested JSON into nice sane dataframes, [`pandas.json_normalize`](https://pandas.pydata.org/pandas-docs/version/1.2.0/reference/api/pandas.json_normalize.html#pandas.json_normalize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>force</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>avon-and-somerset</td>\n",
       "      <td>2021-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bedfordshire</td>\n",
       "      <td>2021-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>btp</td>\n",
       "      <td>2021-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cambridgeshire</td>\n",
       "      <td>2021-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cheshire</td>\n",
       "      <td>2021-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>warwickshire</td>\n",
       "      <td>2018-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>west-mercia</td>\n",
       "      <td>2018-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>west-midlands</td>\n",
       "      <td>2018-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>west-yorkshire</td>\n",
       "      <td>2018-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>wiltshire</td>\n",
       "      <td>2018-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1543 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  force     date\n",
       "0     avon-and-somerset  2021-10\n",
       "1          bedfordshire  2021-10\n",
       "2                   btp  2021-10\n",
       "3        cambridgeshire  2021-10\n",
       "4              cheshire  2021-10\n",
       "...                 ...      ...\n",
       "1538       warwickshire  2018-11\n",
       "1539        west-mercia  2018-11\n",
       "1540      west-midlands  2018-11\n",
       "1541     west-yorkshire  2018-11\n",
       "1542          wiltshire  2018-11\n",
       "\n",
       "[1543 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "availability = pandas.json_normalize(x, 'stop-and-search', 'date').rename(columns={0:'force'})\n",
    "availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [police API documentation](https://data.police.uk/docs/method/stops-force/) tells us how to request records for a given police force and month: using a url of the form\n",
    "```\n",
    "https://data.police.uk/api/stops-force?force=avon-and-somerset&date=2017-01\n",
    "```\n",
    "Let's fetch them all. For each item we'll fetch the data, turn it into a dataframe (again using `pandas.json_normalize`), and then we'll bind everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.police.uk/api/stops-force?force=devon-and-cornwall&date=2021-10 ...\r"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "BASE_URL = 'https://data.police.uk/api/'\n",
    "STOPSDATA_URL = urllib.parse.urljoin(BASE_URL, 'stops-force')\n",
    "availability['url'] = [STOPSDATA_URL + '?' + urllib.parse.urlencode({'force':f, 'date':d}) \n",
    "                       for f,d in zip(availability.force, availability.date)]\n",
    "\n",
    "def get_dataframe(q):\n",
    "    url,date,force = (q.url, q.date, q.force)\n",
    "    print(url + \" ...\", end=\"\\r\")\n",
    "    response = requests.get(url)\n",
    "    x = response.json()\n",
    "    df = pandas.json_normalize(x, sep='_')\n",
    "    df.insert(0, 'month', date)\n",
    "    df.insert(0, 'force', force)\n",
    "    return df\n",
    "\n",
    "# only fetch a little, for illustration purposes\n",
    "df = [get_dataframe(r) for i,r in availability.iloc[:10].iterrows()]\n",
    "stopsearch = pandas.concat(df, axis=0, ignore_index=True, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>force</th>\n",
       "      <th>month</th>\n",
       "      <th>age_range</th>\n",
       "      <th>outcome</th>\n",
       "      <th>involved_person</th>\n",
       "      <th>self_defined_ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>legislation</th>\n",
       "      <th>outcome_linked_to_object_of_search</th>\n",
       "      <th>datetime</th>\n",
       "      <th>...</th>\n",
       "      <th>officer_defined_ethnicity</th>\n",
       "      <th>type</th>\n",
       "      <th>operation_name</th>\n",
       "      <th>object_of_search</th>\n",
       "      <th>outcome_object_id</th>\n",
       "      <th>outcome_object_name</th>\n",
       "      <th>location_latitude</th>\n",
       "      <th>location_street_id</th>\n",
       "      <th>location_street_name</th>\n",
       "      <th>location_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2595</th>\n",
       "      <td>city-of-london</td>\n",
       "      <td>2021-10</td>\n",
       "      <td>18-24</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>Other ethnic group - Not stated</td>\n",
       "      <td>Male</td>\n",
       "      <td>Police and Criminal Evidence Act 1984 (section 1)</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-10-01T06:16:14+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Person search</td>\n",
       "      <td>None</td>\n",
       "      <td>Stolen goods</td>\n",
       "      <td>bu-no-further-action</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>51.516571</td>\n",
       "      <td>587580.0</td>\n",
       "      <td>On or near New Fetter Lane</td>\n",
       "      <td>-0.108164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2186</th>\n",
       "      <td>cheshire</td>\n",
       "      <td>2021-10</td>\n",
       "      <td>25-34</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>White - English/Welsh/Scottish/Northern Irish/...</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-10-04T10:30:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>White</td>\n",
       "      <td>Person and Vehicle search</td>\n",
       "      <td>None</td>\n",
       "      <td>Controlled drugs</td>\n",
       "      <td>bu-no-further-action</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>53.278951</td>\n",
       "      <td>570243.0</td>\n",
       "      <td>On or near Park/Open Space</td>\n",
       "      <td>-2.865144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>btp</td>\n",
       "      <td>2021-10</td>\n",
       "      <td>25-34</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>True</td>\n",
       "      <td>Black/African/Caribbean/Black British - Caribbean</td>\n",
       "      <td>Male</td>\n",
       "      <td>Misuse of Drugs Act 1971 (section 23)</td>\n",
       "      <td>False</td>\n",
       "      <td>2021-10-29T20:00:00+00:00</td>\n",
       "      <td>...</td>\n",
       "      <td>Black</td>\n",
       "      <td>Person search</td>\n",
       "      <td>None</td>\n",
       "      <td>Controlled drugs</td>\n",
       "      <td>bu-no-further-action</td>\n",
       "      <td>A no further action disposal</td>\n",
       "      <td>51.528000</td>\n",
       "      <td>1486616.0</td>\n",
       "      <td>West Ham (Dlr)</td>\n",
       "      <td>0.005042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               force    month age_range                       outcome  \\\n",
       "2595  city-of-london  2021-10     18-24  A no further action disposal   \n",
       "2186        cheshire  2021-10     25-34  A no further action disposal   \n",
       "1773             btp  2021-10     25-34  A no further action disposal   \n",
       "\n",
       "      involved_person                             self_defined_ethnicity  \\\n",
       "2595             True                    Other ethnic group - Not stated   \n",
       "2186             True  White - English/Welsh/Scottish/Northern Irish/...   \n",
       "1773             True  Black/African/Caribbean/Black British - Caribbean   \n",
       "\n",
       "     gender                                        legislation  \\\n",
       "2595   Male  Police and Criminal Evidence Act 1984 (section 1)   \n",
       "2186   Male              Misuse of Drugs Act 1971 (section 23)   \n",
       "1773   Male              Misuse of Drugs Act 1971 (section 23)   \n",
       "\n",
       "     outcome_linked_to_object_of_search                   datetime  ...  \\\n",
       "2595                              False  2021-10-01T06:16:14+00:00  ...   \n",
       "2186                               None  2021-10-04T10:30:00+00:00  ...   \n",
       "1773                              False  2021-10-29T20:00:00+00:00  ...   \n",
       "\n",
       "     officer_defined_ethnicity                       type operation_name  \\\n",
       "2595                     White              Person search           None   \n",
       "2186                     White  Person and Vehicle search           None   \n",
       "1773                     Black              Person search           None   \n",
       "\n",
       "      object_of_search     outcome_object_id           outcome_object_name  \\\n",
       "2595      Stolen goods  bu-no-further-action  A no further action disposal   \n",
       "2186  Controlled drugs  bu-no-further-action  A no further action disposal   \n",
       "1773  Controlled drugs  bu-no-further-action  A no further action disposal   \n",
       "\n",
       "     location_latitude location_street_id        location_street_name  \\\n",
       "2595         51.516571           587580.0  On or near New Fetter Lane   \n",
       "2186         53.278951           570243.0  On or near Park/Open Space   \n",
       "1773         51.528000          1486616.0              West Ham (Dlr)   \n",
       "\n",
       "     location_longitude  \n",
       "2595          -0.108164  \n",
       "2186          -2.865144  \n",
       "1773           0.005042  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopsearch.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a website with xpath<span id=\"xpath\"></span>\n",
    "\n",
    "There are fascinating stories to be discovered from public data, and sometimes you have to work to scrape it from web pages. Here's [an acount](https://onlinejournalismblog.com/2016/11/29/how-the-bbc-england-data-unit-scraped-airport-noise-complaints/) by a BBC data journalist. We'll work with a very simple example: extracting results of the Oxford / Cambridge boat race from the [Wikipedia table](https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results#Main_race).\n",
    "\n",
    "I recommend using [XPath queries](https://www.w3.org/TR/xpath20/) from the [`lxml`](http://lxml.de/) module.\n",
    "XPath is a powerful mini-language for extracting data from hierarchical documents, with wide support across many programming languages &mdash; think of it as regular expressions but for html rather than plain text. If you want to scrape websites then it's worth finding a tutorial and learning XPath. For this course, we'll just see how to use XPath in Python.\n",
    "\n",
    "The first step is to install `lxml`, which is not included with Python.\n",
    "```\n",
    "!pip install lxml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fetch the web page and parse the contents. Most web pages are badly-formatted html (sections not properly closed, etc.), and `lxml.html.fromstring` makes a reasonable attempt to make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "BOATRACE_URL = 'https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results'\n",
    "resp = requests.get(BOATRACE_URL)\n",
    "doc = lxml.html.fromstring(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us `doc`, the root `<html>` element, which we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "2\n",
      "['head', 'body']\n",
      "{'class': 'client-nojs', 'lang': 'en', 'dir': 'ltr'}\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(doc.tag)               # the type of element\n",
    "print(len(doc))              # the number of children\n",
    "print([n.tag for n in doc])  # tags of its children, <head> and <body>\n",
    "print(doc.attrib)            # get the attributes, e.g. <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
    "print(doc.text, doc.tail)    # any text directly under in this element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pull out a particular element from the document, namely the table with boat race results, so we need to work out how to refer to it in XPath. The Chrome webbrowser has a handy tool to help with this. Go to the page you're interested in, and click on &hellip; | More tools | Developer Tools. Click on the element-selector button at the top left:\n",
    "<img src=\"res/xpath1.png\" alt=\"use 'select element' mode\">\n",
    "Go back to the web page, and click on a piece close to what you want to select. I clicked on the top left cell of the table:\n",
    "<img src=\"res/xpath2.png\" alt=\"click roughly where you want\" style=\"height:8em\">\n",
    "Go back to the developer tools window, and navigate to the exact element you want. Here, we want the table. Right-click and choose Copy | Copy XPath. \n",
    "<img src=\"res/xpath3.png\" alt=\"copy the XPath of the element you want\" style=\"height:8em\">\n",
    "It gave me the XPath location `\"//*[@id=\"mw-content-text\"]/div[1]/table[2]\"`. Now we can extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr>\n",
      "<td align=\"center\"><a href=\"/wiki/The_Boat_Race_1841\" title=\"The Boat Race 1841\">5</a></td>\n",
      "<td align=\"center\"><span data-sort-value=\"000000001841-04-14-0000\" style=\"white-space:nowrap\">14 April 1841</span> <img alt=\"double-dagger\" src=\"//upload.wikimedia.org/wikipedia/commons/f/f9/Double-dagger-14-plain.png\" decoding=\"async\" width=\"9\" height=\"14\" data-file-width=\"9\" data-file-height=\"14\"/></td>\n",
      "<td style=\"background:#B7E1E4; color:#000;\" align=\"center\">Cambridge</td>\n",
      "<td align=\"center\">32:03</td>\n",
      "<td align=\"center\">22 lengths</td>\n",
      "<td align=\"center\">1</td>\n",
      "<td align=\"center\">4\n",
      "</td></tr>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 June 1829</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17 June 1836</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 April 1839</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15 April 1840</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14 April 1841</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               t     winner\n",
       "0   10 June 1829     Oxford\n",
       "1   17 June 1836  Cambridge\n",
       "2   3 April 1839  Cambridge\n",
       "3  15 April 1840  Cambridge\n",
       "4  14 April 1841  Cambridge"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick out the table element.\n",
    "# (XPath queries return lists, but I only want one item, hence the [0].)\n",
    "table = doc.xpath('//*[@id=\"mw-content-text\"]/div[1]/table[2]')[0]\n",
    "\n",
    "# Get a list of all rows i.e. <tr> elements inside the table.\n",
    "# Print one, to check things look OK.\n",
    "rows = table.xpath('.//tr')\n",
    "print(lxml.etree.tostring(rows[5], encoding='unicode'))\n",
    "\n",
    "# Extract the timestamp and winner columns.\n",
    "# The timestamp is in the second child, in a <span> element with a \"data-sort-value\" attribute.\n",
    "# The winner is in the third child.\n",
    "df = {'t': [row[1].xpath('.//span[@data-sort-value]')[0].text for row in rows[1:]],\n",
    "      'winner': [row[2].text for row in rows[1:]]}\n",
    "df = pandas.DataFrame(df)\n",
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consider the ethics of your web scraping. Here are some thoughts:\n",
    "from [Sophie Chou at the MIT Media Lab](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/), and the [data journalist N&auml;el Shiab](https://gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying an SQL database<span id=\"sql\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your data is in an SQL database, access is easy. Here's an example with Postgresql, a dialect of SQL.\n",
    "\n",
    "Databases are usually secured, and you need various credentials to log in. \n",
    "It's good practice to store our code in a repository, but bad practice to store credentials\n",
    "there too. Instead, we can store credentials in separate file that isn't checked in to the repository.\n",
    "I like to store credentials in a JSON file, something like this:\n",
    "```\n",
    "{\"user\": \"SPQR\", \"password\": \"TOPSECRET\", \"host\": \"***\", \"dbname\": \"***\"}\n",
    "```\n",
    "which is easy to load into Python as a dictionary. Then I can use the fields of this dictionary as arguments to `psycopg2.connect`, to establish the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2    # module for connecting to a Postgresql database\n",
    "import json        # standard module for reading json files\n",
    "\n",
    "creds = json.load(open('res/secret_creds.json'))\n",
    "conn = psycopg2.connect(**creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run arbitrary SQL queries, and retrieve the results as a pandas dataframe. Pass in parameters with the `%(name)s` quoting mechanism, to keep ourselves safe from SQL injection attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uri</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>catchment</th>\n",
       "      <th>river</th>\n",
       "      <th>town</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>index</th>\n",
       "      <th>measure_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>station_uri</th>\n",
       "      <th>qualifier</th>\n",
       "      <th>parameter</th>\n",
       "      <th>period</th>\n",
       "      <th>unit</th>\n",
       "      <th>valuetype</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>E21778</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>52.061730</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>397</td>\n",
       "      <td>398</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>52113</td>\n",
       "      <td>Parrett, Brue and West Somerset Streams</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>51.023159</td>\n",
       "      <td>-2.565568</td>\n",
       "      <td>918</td>\n",
       "      <td>919</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1272</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cambridge Baits Bite</td>\n",
       "      <td>E60101</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Milton</td>\n",
       "      <td>52.236542</td>\n",
       "      <td>0.176925</td>\n",
       "      <td>1454</td>\n",
       "      <td>1455</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                uri  \\\n",
       "0    345  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1    800  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2   1272  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                  label      id                                 catchment  \\\n",
       "0     Great Chesterford  E21778  Cam and Ely Ouse (Including South Level)   \n",
       "1      Weston Bampfylde   52113   Parrett, Brue and West Somerset Streams   \n",
       "2  Cambridge Baits Bite  E60101  Cam and Ely Ouse (Including South Level)   \n",
       "\n",
       "       river               town        lat       lng  index  measure_id  \\\n",
       "0  River Cam  Great Chesterford  52.061730  0.194279    397         398   \n",
       "1  River Cam   Weston Bampfylde  51.023159 -2.565568    918         919   \n",
       "2  River Cam             Milton  52.236542  0.176925   1454        1455   \n",
       "\n",
       "                                                 uri  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                         station_uri qualifier    parameter  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "\n",
       "   period  unit      valuetype    low   high  \n",
       "0   900.0     m  instantaneous  0.109  0.333  \n",
       "1   900.0     m  instantaneous  0.026  0.600  \n",
       "2   900.0  mASD  instantaneous  0.218  0.294  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''\n",
    "SELECT *\n",
    "FROM flood_stations AS s JOIN flood_measures AS m ON (m.station_uri = s.uri) \n",
    "WHERE river = %(river)s OR town = %(town)s\n",
    "LIMIT 3\n",
    "'''\n",
    "pandas.read_sql(cmd, conn, params={'river': 'River Cam', 'town': 'Cambridge'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
